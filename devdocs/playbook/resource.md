# Playbook for Contributing New Resources

## Overview

The Power Platform Terraform Provider manages Power Platform components (environments, policies, users, etc.) as Terraform resources. Each resource in this provider is implemented using Terraform’s **Plugin Framework**, not the legacy SDK. In practice, this means defining a resource with a schema and CRUD (Create, Read, Update, Delete) operations as Go code that interfaces with Power Platform’s REST APIs. Resources are organized by service domain within the provider’s repository – for example, an environment resource lives under an “environment” service module. Terraform resources in this provider follow a consistent naming convention (all start with `powerplatform_` and correspond to a Power Platform object) and a similar structure: a schema describing all configurable fields, and methods to map Terraform operations to Power Platform API calls. This structure ensures that when a user writes a Terraform configuration, the provider knows how to translate it into API requests and keep Terraform state in sync with actual Power Platform resources.

## Principles and Rules

When adding a new resource, adhere to the coding conventions and patterns used throughout the existing provider:

- **Plugin Framework patterns** – Follow HashiCorp’s Terraform Plugin Framework guidelines for resources. Define resources as Go structs implementing the `resource.Resource` interface (with methods like `Schema`, `Create`, `Read`, etc.). Avoid using deprecated SDKv2 practices. For example, use the framework’s attribute types and diagnostics system instead of old `schema.ResourceData`. The provider’s codebase consistently uses these patterns, ensuring all resources handle state and data in a uniform way.
- **Repository structure** – Place new resource code in a relevant service folder under `internal/services/<service_name>`. The provider separates resource logic by service (e.g., “environment”, “data_loss_prevention”) for clarity. If your resource pertains to a new service area, create a new folder; otherwise, add to an existing one. This separation keeps resource code focused on Terraform-specific logic, while any HTTP API client code or helpers can also reside in that service package.
- **Clear schema definitions** – Maintain consistency in how schemas are defined. Use descriptive names and include both `Description` and `MarkdownDescription` for each attribute so that documentation is auto-generated clearly. For instance, the **ID** field is always a computed string with a description like “Unique id (GUID)”. Required fields like names or types should be clearly marked `Required: true`. Optional settings or complex sub-objects should be marked `Optional: true` or use nested attribute types. This consistent schema styling is evident across the repo and helps users understand resource configuration.
- **Force replacement vs. in-place update** – Identify which resource attributes require recreation on change and mark them accordingly. In this provider, such attributes are typically annotated with Terraform plan modifiers. For example, if a field cannot be changed without destroying and re-creating the resource (such as an environment’s location or type), use the `RequiresReplace()` plan modifier in the schema. The codebase uses `stringplanmodifier.RequiresReplace()` for fields where changing them triggers a new resource (analogous to “ForceNew” in the old SDK). By following this practice, you ensure Terraform creates a plan that replaces the resource when those fields change, preventing improper API calls.
- **Consistency and reuse** – Reuse existing utilities and patterns from the repo. For instance, use common validation logic or constants if available (e.g., regex for GUIDs or allowed enums). The repository might have helper functions (often under `internal/common` or service packages) for tasks like parsing IDs or handling pagination. Leverage these instead of writing from scratch. Similarly, follow naming conventions (method names, error messages, file names) used in existing resources for a uniform codebase.

By following these principles, your contribution will align with the style and quality of the provider’s code. Always run `make fmt` and `make lint` (if available) to conform to Go formatting and linting rules, and consult the repository’s **Contributing Guide** for any additional conventions.

## Step-by-Step Guide

Creating a new Terraform resource involves several stages. Below is a step-by-step process, from defining the schema to implementing operations, integrating with the API, and handling state:

1. **Define the Resource Schema** – Begin by defining the schema that describes the resource’s attributes. In your resource’s `Schema(ctx, req, resp)` method, return a `schema.Schema` with all the attributes this resource should manage. Each attribute is defined using the Plugin Framework’s attribute types (e.g. `schema.StringAttribute`, `schema.Int64Attribute`, `schema.ListAttribute`, etc.). Mark required fields and computed fields appropriately. For example, a typical ID field is defined as computed string with a plan modifier to carry over its prior value until set on creation. Required inputs (like an environment’s `display_name` or `environment_type`) should be marked `Required: true` and given proper descriptions. If an attribute is optional or computed by the API, mark it `Optional: true` or `Computed: true` as needed. Use nested attribute objects for complex sub-resources: for instance, the `powerplatform_environment` resource has a nested `dataverse` block with its own schema for database settings. You can define this using `schema.SingleNestedAttribute` or `schema.ListNestedAttribute` containing a map of sub-attributes. Be sure to include any **timeouts** block if the operation can be long-running – the provider uses a nested `timeouts` schema (with `create`, `update`, `delete` fields of type string) to allow user customization of operation timeouts. Lastly, attach validators or plan modifiers to attributes where appropriate. For example, if only certain values are allowed, use a validator (from `terraform-plugin-framework-validators`) to check the value, or if an attribute is sensitive (like a secret), mark it accordingly. By carefully designing the schema, you create the contract for what Terraform configurations of this resource can include.

2. **Implement CRUD Operations** – With the schema defined, implement the Create, Read, Update, and Delete methods of the resource. These methods translate Terraform intents into Power Platform API calls:
   - **Create:** In the `Create(ctx, req, resp)` method, retrieve the desired state from `req.Plan`. The Plugin Framework allows you to easily decode the plan into a Go struct or read individual attributes. For example, you might define a Go struct for your resource’s state (for clarity) and use `req.Plan.Get` to populate it. Then, call the appropriate Power Platform API to create the resource. This often involves constructing a request object or URL with the data from the plan. The provider code typically delegates to a service client here – e.g., calling a function in the `internal/services/<service>` package that wraps the HTTP POST call. Ensure you handle any asynchronous behavior: if the API returns immediately with an operation ID (common in Power Platform for environment provisioning), your Create should poll until the resource is actually ready. You can use the Terraform context’s timeout (from the `timeouts` schema or the context deadline) to limit your polling loop. After a successful creation, extract the new resource’s identifier (GUID or key) from the API response and set it in the Terraform state. For example, set the `ID` attribute with `resp.State.Set` or via the state struct. Also set any attributes that the API returns on creation. Many resources perform a follow-up **Read** after Create to populate all fields – you can either call your Read method explicitly or manually set known attributes if the create response provided them. The key is to end `Create` with a fully populated state so Terraform knows the actual resource settings.
   - **Read:** In `Read(ctx, req, resp)`, your goal is to refresh Terraform state with the latest data from Power Platform. Use the ID from state (`req.State`) to fetch the resource via the API (e.g., GET request to the resource endpoint). If the resource is found, map the response to the Terraform state fields and set them (`resp.State.Set(...)`). If the resource no longer exists (the API returns 404), handle this gracefully by telling Terraform the resource is gone. In the Plugin Framework, you can call `resp.State.RemoveResource(ctx)` to mark it as deleted. The provider code should use diagnostics to report if a read fails unexpectedly: e.g., `resp.Diagnostics.AddError("Error reading resource", err.Error())` if an API call fails for a reason other than Not Found. Reading should never modify anything in the remote system – it only queries and updates state. It’s common to share conversion logic between Create/Update and Read (for example, parsing API responses), possibly via helper functions.
   - **Update:** In `Update(ctx, req, resp)`, handle changes to the resource’s configuration. The `req.Plan` contains the desired state and `req.State` has the prior state. Determine which attributes have changed (the Plugin Framework can detect differences, or you can compare plan vs state values). Update logic will vary: some resources might map to a single "Update" API call that accepts a partial payload of changed fields, while others might require multiple calls or even a full replace. For each updatable field, call the corresponding API (e.g., an HTTP PATCH or PUT). Use the resource’s service client functions to perform these calls. If the provider has marked certain fields with `RequiresReplace` (force new), Terraform will actually call Delete then Create instead of Update when those fields change, so you typically don’t handle those in Update (they won’t appear in a plan that calls Update). After applying updates, you may again call the GET API (or reuse the Read method) to refresh the state. This ensures any server-generated values or defaults are captured. Handle partial failure carefully – if one API call fails, report the error and try not to leave Terraform state in a confused state. (Usually, Terraform will continue to show the prior state until a successful update sets a new state.)
   - **Delete:** In `Delete(ctx, req, resp)`, use the resource ID to call the API that deletes the resource. This is often an HTTP DELETE request to a specific endpoint. As with create, pay attention to asynchronous deletion if applicable – some deletions might return an operation ID and require polling until completion. Use the context timeout to avoid hanging indefinitely. If the API indicates the resource is already gone, you can treat it as success (Terraform will consider the resource destroyed even if it wasn’t present). Once the delete call (and any necessary polling) succeeds, you don’t need to explicitly remove the state – returning from Delete without error is enough for Terraform to clean up state. However, if any error occurs that means the resource wasn’t deleted, surface that via `resp.Diagnostics` so Terraform knows the destroy failed. Always test deletion of your resource to ensure no leftover bits remain on the platform and that Terraform properly updates its state file.

3. **Handle API Interactions** – The provider’s resources act as a bridge to the Power Platform REST APIs. It’s important to follow the repository’s patterns for API calls. Typically, the provider uses a **client** that’s configured in the Provider’s `Configure` method (holding authentication tokens, base URLs, etc.), which is passed to resources via `req.ProviderData`. In your resource code, retrieve this client in the resource’s `Configure` or in each CRUD call. For example, many resources implement the `resource.ResourceWithConfigure` interface so that the framework will call a `Configure(ctx, req, resp)` on your resource implementation. In that method, you can cast `req.ProviderData` to the Power Platform API client and store it in the resource struct for later use. By doing so, your CRUD methods can easily call the client’s methods (e.g., `client.CreateEnvironment(...)`). **Do not hard-code credentials or create new HTTP clients in each method** – use the provided client to inherit the provider’s authentication context. When constructing API requests, use the common helper functions and types defined in the repo. For instance, if there’s a function to build a resource URL or a struct that represents the JSON payload, use those to maintain consistency. The repo’s service packages often have functions like `CreateX`, `GetX`, `UpdateX` that encapsulate the REST calls. Calling those from your CRUD methods keeps the code clean. Always check for errors from client calls and convert them to Terraform diagnostics. If the API returns an error response, capture the message to help the user troubleshoot. For example, if attempting to create a resource yields a 400 error with a message, include that in an `resp.Diagnostics.AddError` so the Terraform user sees the API message. Also handle rate limiting or throttling if relevant – the Power Platform API might return 429 or similar; a good practice is to implement retries with backoff for such cases (the framework doesn’t do this automatically, but you can catch and retry a few times, or utilize any retry helpers in the repo). In summary, integrate with the API through the established client and helper functions, check all call results, and ensure no response (especially errors) goes unhandled.

4. **Add Validation and Error Handling** – Incorporate validation logic to prevent invalid configurations and to fail fast on user errors. The Terraform Plugin Framework allows adding **Validators** to schema attributes. The Power Platform provider uses the `terraform-plugin-framework-validators` library for common checks. For example, if an attribute must be a valid GUID, you could add a regex or custom validator to check the format and return a user-friendly error if it’s not valid. If only specific strings are allowed (e.g., an enum like environment type must be "Production" or "Sandbox"), use a validator to enforce that. By validating inputs in the Terraform provider, you can catch mistakes before sending requests to the API. Implement validation both declaratively (via schema validators) and imperatively if needed. Imperative validation might occur in Create/Update: for instance, if two fields are mutually exclusive or one requires the other, you can check the plan and raise a diagnostic error in `Create` if the combination is wrong. Follow the patterns in the repo for error handling: when adding an error diagnostic, provide a clear `summary` and `detail`. The summary is a short message and detail can include more context or the raw error from the API. For example, an error handling snippet might look like: `resp.Diagnostics.AddError("Failed to create Environment", "The Power Platform API returned an error: "+ err.Error())`. This pattern is used throughout the provider to surface errors. Additionally, ensure that your resource handles partial state updates carefully. If an error occurs during Create after the resource was actually created on the server, you should still set the ID in state (so that a subsequent Terraform run can detect and possibly import or remove the dangling resource). However, this situation is rare; usually you can treat a failure in Create as meaning no resource was created (the API is transactional). In Update, if a failure occurs, Terraform will continue to use the old state (since you don’t call `resp.State.Set` on error), which is normally correct. In all cases, do not panic or throw exceptions – use the `diagnostics` to report errors. This aligns with how the provider’s existing resources handle errors and makes debugging easier for users.

5. **Manage State Persistence** – Properly managing state means that after any CRUD operation, the Terraform state should reflect the true state of the resource. This involves setting all the necessary attributes in `resp.State` during Create/Update/Read. A common practice in the provider is to define a Go struct type that mirrors the resource schema (often called a “resource model”). You fill this struct from API responses and then call `resp.State.Set(ctx, &model)` to save it. This approach reduces mistakes, because the struct fields and schema attributes stay in sync. For example, in the Data Record resource, after creating a record the code populates a struct with the new record’s fields (id, environment_id, table, etc.) and saves it. Always include the `id` in state once known – without it, Terraform cannot track the resource. The provider typically uses the `UseStateForUnknown` plan modifier on the ID attribute so that during planning the ID is left unknown until the Create sets it. For computed attributes (ones the API fills in), ensure they get updated in Read so that a subsequent `terraform plan` doesn’t show a drift. For instance, if the API assigns a default or a generated URL, store that in state. If your resource has nested attributes (maps or lists), reconstruct them in the state exactly as Terraform expects (respect the schema types). Mismanaged state is a common source of bugs – e.g., leaving out a field might cause Terraform to constantly think it needs to update it. A tip is to run `terraform plan` after an apply to verify that no unexpected changes are detected; this confirms your state is complete. Also consider the **import** scenario: if possible, implement an ImportState function or ensure that Read can populate state given only an ID (this helps users import existing resources). While import is optional, it’s a good practice if the API allows retrieving a resource by ID. In summary, always sync Terraform state to reality: after any create/update, the in-memory state should be set to what the Power Platform reports, and after deletion, the state should be cleared.

## Testing and Debugging

Once you have implemented the new resource, it’s crucial to verify its correctness through tests and troubleshoot any issues:

- **Unit Testing**: Write unit tests for your resource’s logic focusing on the Terraform side (without requiring real Power Platform access). In the repository, each resource or data source has a corresponding `_test.go` file under `internal/` (or its service folder). Follow the existing examples to create tests for your resource’s CRUD behaviors. Aim to cover the “happy path” – creating the resource with valid input, reading it, updating it, and deleting it – as well as edge cases if any. The provider uses HTTP request mocking to simulate Power Platform API responses in unit tests. Specifically, it leverages a library to intercept HTTP calls and return predefined JSON. You should add new mock responses for your resource’s API endpoints. Place these JSON fixtures under `internal/services/<your_service>/test/<resource>/<test_name>/...` as per the project structure. You can often copy and anonymize real API responses for use as mocks (ensuring no sensitive data is included). The repo provides some common mock setup functions (e.g., `ActivateEnvironmentHttpMocks`) that you can reuse to initialize mocks for authentication and environment setup. Use those to avoid duplicating boilerplate. In your unit tests, use Terraform’s testing framework (`terraform-plugin-testing` package) to create test steps. For example, you might write a TestCase with `ProtoV6ProviderFactories` (provided by the repo’s test helpers) and define `resource.TestStep` structs that have a `Config` (HCL defining your resource) and then `Check` functions to verify the state. The repository’s tests often use `resource.TestCheck*` functions to assert that certain attributes in state match expected values (or regex). Emulate those patterns so your tests integrate well. Run unit tests with `make unittest` (which runs all tests with `TF_ACC=0`). Ensure all your new tests pass before moving on.

- **Acceptance Testing**: The provider includes acceptance tests which actually call the live Power Platform APIs (these are the tests requiring `TF_ACC=1`). For every unit test scenario, there should ideally be a corresponding acceptance test. The acceptance tests live alongside unit tests but are only run when `TF_ACC` is set. You can create them in the same `_test.go` file, typically guarded by a check on `TF_ACC`. These tests will use real credentials and actually create/delete resources in a test tenant, so use them carefully. Write acceptance tests similar to unit tests, but they won’t use mocks – instead, they rely on real API interactions. The config and checks can often be the same as the unit test (since the desired outcome is the same). Before running them, you need to configure your environment for authentication (see the project’s documentation on credentials). Usually this means setting environment variables like `POWER_PLATFORM_CLIENT_ID`, `POWER_PLATFORM_CLIENT_SECRET`, etc., or ensuring `az login` is done if using Azure CLI auth. Also, make sure you have appropriate permissions in the target tenant, as the tests will create and destroy actual resources (e.g., permission to create an environment if you are testing the environment resource). Run acceptance tests with `make acctest` to execute the whole suite, or run a specific test using Go’s `-run` flag (e.g., `TF_ACC=1 go test -v ./... -run TestAccYourResourceName`). These tests will take longer and may incur costs or resource usage in your Power Platform environment, so it’s wise to clean up any leftovers if a test fails midway. By writing and executing acceptance tests, you validate that your resource code works end-to-end against real Power Platform APIs.

- **Debugging and Common Issues**: During development or test failures, you may need to debug the provider. The repository is set up for debugging via VS Code devcontainers. You can launch the provider in debug mode (for example, by pressing F5 in VS Code with the devcontainer, as noted in the Developer Guide). This will build the provider and run it; the debug console will show a `TF_REATTACH_PROVIDERS` environment variable. You can copy that and use it in a separate terminal where you run `terraform apply` on a test configuration; it will attach the running debugger to Terraform, allowing you to hit breakpoints in your Go code. This is extremely useful for stepping through Create/Read/Update calls and inspecting variables. If you prefer not to use an IDE, you can insert `tflog.Debug` statements or plain `log.Printf` (though the latter is not idiomatic in providers) to trace execution. To see provider logs at runtime, set the environment variable `TF_LOG` to `DEBUG` (or `TRACE` for even more detail) when running Terraform. Terraform will then output verbose logs from the provider plugin to stderr. Look for your resource's log messages or any errors/stack traces there. Common issues to watch for include: schema mismatches (if you forget to set a computed field in state, you'll see Terraform plan wanting to change it each run), improper error handling (crashes or non-informative errors), and API nuances (e.g., the API might return a field in a different format than you assumed, causing state mismatches). Use the Terraform CLI `terraform plan` and `terraform apply` liberally against a real Power Platform to manually verify behavior. For instance, after creating your resource, run another `terraform plan` to ensure it reports "No changes" (if it reports changes you didn't expect, you may be missing something in state or schema). If your resource involves asynchronous operations, pay close attention to timeouts – if Terraform reports a timeout error, you may need to adjust the default or handle the operation's completion more robustly. Finally, if you encounter specific issues (like the dynamic set bug that caused an unexpected plan in the data record resource), consider searching the repository's issue tracker or HashiCorp discuss forums; chances are others have hit similar problems. Addressing these, either via code changes or proper documentation, will make your resource robust.

- **Validation and Documentation**: Before submitting your contribution, validate that the resource works in practice and update docs. Run `terraform validate` and `terraform apply` on example configurations of your new resource to ensure everything ties together. The repository requires that you update or add example Terraform files under the `examples/` directory for your resource, demonstrating usage. Test those examples manually. Also generate the documentation by running `make docs`, which will update the markdown files under `docs/` (they are derived from your schema and descriptions). Check that the generated docs (or any handwritten ones) properly explain the resource and its arguments. This is part of "validating" your new resource – making sure it's not only working correctly but also user-ready. Once tests are passing and docs are updated, you can open a pull request following the project's contribution workflow.

By following this detailed guide, you will align with the repository's standards and create a high-quality Terraform resource. Adhering to the Terraform Plugin Framework best practices and the Power Platform provider's established patterns will ease the review process and result in a reliable new resource for users. Good luck with your development, and happy contributing!

**Sources:**

- Power Platform Terraform Provider Developer Guide
- Power Platform Provider Repository (Schema and code examples)
- HashiCorp Terraform Plugin Framework Documentation and Terraform Testing Guidelines
